{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF06URYej2Af"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from datasets import DatasetDict\n",
        "\n",
        "# Assuming you have a DataFrame named 'my_dataframe'\n",
        "\n",
        "df = pd.read_csv('complete_dataset_labelled (1).csv')\n",
        "\n",
        "\n",
        "df['text'] = df['text'].dropna()\n",
        "\n",
        "\n",
        "t1=[]\n",
        "for x in df['text']:\n",
        "  y1=re.sub(r'[^\\x00-\\x7F]+',' ', x)\n",
        "  t1.append(y1)\n",
        "\n",
        "df['text']=t1\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "\n",
        "def removing_unwanted_data(text):\n",
        "    # Preprocessing to remove unwanted characters and links\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\<a href', ' ', text)\n",
        "    text = re.sub(r'&amp;', '', text)\n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "    text = re.sub(r'\\[[0-9]*\\]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
        "    text = re.sub(\"[^A-Z]\", \" \", text, 0, re.IGNORECASE)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove Norwegian stop words\n",
        "    norwegian_stopwords = set(stopwords.words('norwegian'))\n",
        "    tokens = [word for word in tokens if word not in norwegian_stopwords]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = SnowballStemmer(\"norwegian\")\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # Remove words with less than 2 characters\n",
        "    filtered_tokens = [token for token in stemmed_tokens if len(token) > 1]\n",
        "\n",
        "    # Rejoin tokens into a single string\n",
        "    text = ' '.join(filtered_tokens)\n",
        "\n",
        "    return text\n",
        "\n",
        "temp=df['text']\n",
        "df['text']= list(map(removing_unwanted_data,df['text']))\n",
        "\n",
        "\n",
        "# Assuming your original DataFrame looks like this: df[\"star_rating\"] = [1, 2, 3, 4, 5, 1, 2, ...]\n",
        "# Remap the values in the \"star_rating\" column\n",
        "\n",
        "df[\"category\"] = df[\"category\"].replace({1: 0, 2: 1, 3: 2, 4: 3 , 5: 4})\n",
        "df[\"category\"].value_counts()\n",
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import fasttext\n",
        "\n",
        "# Assuming df is your DataFrame with 'combined_text' and 'star_rating' columns\n",
        "X_texts = df['text']\n",
        "y = df['category']\n",
        "\n",
        "# Train supervised fastText model\n",
        "train_data = df.apply(lambda df: f\"__label__{df['category']} {df['text']}\", axis=1)\n",
        "train_data.to_csv('train_data.txt', index=False, header=False)\n",
        "model_fasttext = fasttext.train_supervised('train_data.txt', epoch=50, lr=0.1)\n",
        "\n",
        "# Convert text to sequences\n",
        "X_sequences = [model_fasttext.get_sentence_vector(text.replace('\\n', '')) for text in X_texts]\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "X_padded = pad_sequences(X_sequences, dtype='float32')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Save Model\n",
        "import fasttext\n",
        "from keras.models import Sequential, save_model, load_model\n",
        "\n",
        "\n",
        "#model_fasttext.save_model(\"model_fasttext.bin\")\n",
        "\n",
        "# Load the model\n",
        "loaded_model_fasttext = fasttext.load_model('model_fasttext.bin')\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Bidirectional, LSTM, GRU, Dense,Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming X_padded is a 2D array, convert it to a 3D array\n",
        "X_padded_3d = np.expand_dims(X_padded, axis=2)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_padded_3d, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model with both Bidirectional LSTM and Bidirectional GRU\n",
        "model_bilstm_gru = Sequential()\n",
        "model_bilstm_gru.add(Bidirectional(LSTM(100, return_sequences=True), input_shape=(X_padded_3d.shape[1], X_padded_3d.shape[2])))\n",
        "model_bilstm_gru.add(Dropout(0.2))\n",
        "model_bilstm_gru.add(Bidirectional(GRU(80, return_sequences=True)))\n",
        "model_bilstm_gru.add(Bidirectional(LSTM(50, return_sequences=True)))  # Adjusted input shape\n",
        "model_bilstm_gru.add(Bidirectional(GRU(30)))\n",
        "model_bilstm_gru.add(Dense(5, activation='relu'))\n",
        "model_bilstm_gru.add(Dense(5, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model_bilstm_gru.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model and collect history for plotting\n",
        "history_bilstm_gru = model_bilstm_gru.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "\n",
        "# Plot the training and validation loss curves\n",
        "plt.plot(history_bilstm_gru.history['loss'], label='Training Loss')\n",
        "plt.plot(history_bilstm_gru.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss (Bidirectional GRU)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and validation accuracy curves\n",
        "plt.plot(history_bilstm_gru.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_bilstm_gru.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy (Bidirectional GRU)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_proba = model_bilstm_gru.predict(X_test)\n",
        "y_pred = [int(i.argmax()) for i in y_pred_proba]\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_prob = model_bilstm_gru.predict(X_test)\n",
        "\n",
        "# Binarize the y_test labels for multi-class ROC AUC calculation\n",
        "y_test_binarized = label_binarize(y_test, classes=[0, 1, 2, 3, 4])\n",
        "\n",
        "# Calculate the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test_binarized, y_pred_prob, multi_class='ovr')\n",
        "print(f\"ROC-AUC Score: {roc_auc}\")\n",
        "\n",
        "\n",
        "# Print accuracy and classification report\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "# Assuming you know your class labels\n",
        "\n",
        "import seaborn as sns  # Import seaborn for plotting\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "class_labels = ['1', '2', '3','4','5']  # Replace with your actual class names\n",
        "\n",
        "# Generate and print the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    }
  ]
}